---
title: "Diamonds: Predicting Price"
author: "David West"
date: "9/25/2020"
output: 
  html_document:
    toc: true
    toc_float: true
    tod_depth: 4
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = F, message = F)
```

# Introduction
The purpose of this project is to create a machine learning model that can predict the price of a diamond based on its charateristics. This report will start with an initial exploratory data analysis (EDA). The EDA will be filled with data visualizations and inital Q&As to help understand a data set. The EDA is then followed by a model exploration. Different machine learning alogrithms will be used to create predictive models. Models will then be compared against each other and the most accurate predictive model will be determined. The conclusion section will provide an executive summary of data insights and the selected predictive model.

# EDA
This inital exploratory analysis will attempt to answer the following questions: <br /><br />

1. First Look: What does the data look like? What are some initial descriptive statistics and insights. <br /><br />

2. Best of the Best: Imagine someone wants to buy the best possible diamond, and money is no object. They only want to consider diamonds in the top categories of cut (Ideal), color (D), and clarity (IF). They want the most ideal range for depth (59-63) and table (54-57). Within the dataset, if we plot carat versus price can we fit a clean trendline? Is it linear? Exponential? What's the price of the largest carat, and is it the most expensive? <br /><br />

3. Depth and Table Percentages: I found the ideal depth and table values mentioned above online, but let's explore the dataset a little. if we fix the 4 C's (carat, cut, color, and clarity), how much do depth and table impact price? If we widen the ranges slightly, can we save a substantial amount? <br /><br />

4. Best Bang for the Buck: Imagine someone wants to find the diamond which maximizes cut, color, and clarity per dollar. Using the expanded depth and table values from question 2 above, when does price start to increase exponentially for cut? What about for color? And clarity? <br /><br />

5. Bigger is Better: Imagine a guy named Bob who wants to buy a pair of diamonds for his wife, and have them made into earrings for her birthday. In Bob's mind, size (carat) is all that matters. He has $3200. He needs Two diamonds with the exact same cut, color, and clarity (with very comparable depth and table values), and he wants them to be as big as possible. What size carat can he afford? If he adjusts his budget, how much does the "maximum carat" size shift? Can we plot that and fit a line to it to find the "knee in the curve"?<br /><br />

## First Look
This data set is provided in the ggplot2 package from CRAN. Let's load the tidyverse collection of packages, which includes ggplot2 and some other packages (like dplyr) which are useful for data manipulation. Let's also load the plotly package, which allows us to create advanced visualizations.
```{r}
# Load libraries needed for EDA
library(tidyverse)
library(plotly)

# save diamonds data into an object called data
data <- ggplot2::diamonds
```

A sample of the data is provided below:
```{r}
head(data)
```
Looks like there is a mix of numeric and categorical data. The numeric data (carat, depth, table, x, y, & z) are doubles (dbl), which means they are numerics with decimal places. Price is a numeric integer, so it will not have any decimal places. The categorical data (cut, color, and clarity) are ordered (ord), which means it has a specific order, such as best to worst. Similar to the sample above, the glimpse function provides a sample of the data in a different format that is more preferable when a data set is very large. 
```{r}
glimpse(data)
```
There are `r nrow(data)` rows of data. Each row is one observation / one diamond. There are `r ncol(data)` columns of data. Each column is a variable / feature. Below is a quick statistical summary of each feature:
```{r}
summary(data)
```
A minimum, 1st quartile, median, mean, 3rd quartile, and maximum is provided for all the numeric data. For example, the median price of diamonds in this data set is `r median(data$price)`. A data count is provided for all categorical data. For example, there are `r data %>% filter(cut=="Ideal") %>% count() %>% pull()` diamonds that are Ideal cut in this data set. Since all the categorical data is ordered, the counts are in its pre-specified order. For the example, the order for color is: `r data %>% arrange(color) %>% select(color) %>% unique() %>% pull()`. Below is a definition for each feature: <br />
<br />

carat: weight of the diamond (`r min(data$carat)` to `r max(data$carat)`) <br />
cut: quality of the cut (in order of worst to best; `r levels(data$cut)`) <br />
color: diamond color (in order of best to worst; `r levels(data$color)`) <br />
clarity: a measurement of how clear the diamond is (in order of worst to best; `r levels(data$clarity)`) <br />
depth: total depth percentage (total height divided by total width) = 2 * z / (x + y) (`r min(data$depth)` to `r max(data$depth)`) <br />
table: width of the top of a diamond relative to its widest point (`r min(data$table)` to `r max(data$table)`) <br />
price: price in US dollars (`r min(data$price)` to `r max(data$price)`) <br />
x: length in mm (`r min(data$x)` to `r max(data$x)`) <br />
y: width in mm (`r min(data$y)` to `r max(data$y)`) <br />
z: depth in mm (`r min(data$z)` to `r max(data$z)`) <br />
<br />
My OCD is kicking in a little bit for color. It is in the order of best to worst, while cut and clarity are in order of worst to best. The script below reorders color from worst to best for consistency. This will have no effect what-so-ever on the analysis, it's just my personal preference.
```{r}
# Reordering color so it's from worst to best. Just like all the other ordered features.
# This is personal preference. It's not a necessary step.
data <- data %>%                  # Create a new object, data, from the old object, data
  mutate(                         # Change its columns
    color = ordered(color,        # Make the color column and ordered column
                    levels = c(   # The order/levels are as follows:
                      "J",
                      "I",
                      "H",
                      "G",
                      "F",
                      "E",
                      "D"
                    )))
```
Now we can get to the interesting stuff. Let's look at how price is affected by each individual feature.

### Price vs Carat
Below is a scatter plot of price vs carat:
```{r}
ggplot(data, aes(x=carat, y=price)) + # Plot the object data, with carat mapped to the x axis and price mapped to the y axis
  geom_point(alpha=0.1)               # Make the plot a scatter plot. Make each point 10% transparent/alpha
```
<br />
Price increases at an increasing rate as carat increases.

### Price vs Cut
Below is a box-plot of price by cut.
```{r}
{ggplot(data, aes(x=cut, y=price)) + # Plot the object data, with cut mapped to the x axis and price mapped to the y axis
  geom_boxplot()} %>%                # Make the plot a box plot
  ggplotly()                         # Make the plot interactive with the ggplotly function
```
<br />
There is no immediately visible trend. I would expect price to increase as the cut becomes better. However, other features may be affecting this box-plot. For example, maybe the average carat size for ideal diamonds is lower than the average carat size for fair. Therefore, the average price for ideal cuts in this data set are lower than fair cuts. This will be explored in more depth later on in this analysis. For now, we are only going to look at price vs each individual feature.

### Price vs Color
Below is a box-plot of price by color.
```{r}
{ggplot(data, aes(x=color, y=price))+
  geom_boxplot()} %>% 
  ggplotly()
```
<br />
Very interesting! As color gets better, price decreases. This absolutely does not pass the common sense test. Similar to cut, other features (like carat) must be affecting these results.

### Price vs Clarity
Below is a box-plot of price by clarity.
```{r}
{ggplot(data, aes(x=clarity, y=price))+
  geom_boxplot()} %>% 
  ggplotly()
```
<br />
These are similar results to color. As clarity gets better, price decreases. Once again, this does not pass the common sense test.

### Price vs Depth Percentage
Below is a scatter plot of price vs depth percentage.
```{r}
ggplot(data, aes(x=depth, y=price))+
  geom_point(alpha=0.1)
```
<br />
It looks like as the range of depth narrows, the price increases. It appears more expensive diamonds are roughly around 58% to 63% depth percentage.

### Price vs Table Percentage
Below is a scatter plot of price vs table percentage.
```{r}
ggplot(data, aes(x=table, y=price))+
  geom_point(alpha=0.1)
```
<br />
This looks similar to depth percentage. As the range of table percentage narrows, the price increases. Although, it doesn't to appear to be as drastic as depth percentage.

### Price vs Length (x)
Below is a scatter plot of price vs length (x).
```{r}
ggplot(data, aes(x=x, y=price))+
  geom_point(alpha=0.1)
```
<br />
This looks similar to carat. Price increases at an increasing rate as length increases. There's also some data points that have a length of zero. These must be missing data points and should be converted to NAs. If they are not, this can mess up some predictive models down the line. First let's find out how many zeros there are.
```{r}
df <- data %>%     # Create a new object df from the object data
  filter(x==0)     # show only the data that has a zero value for x

print(df)          # Display the object df
```
There are `r nrow(df)` rows of data with x labeled as zero and there are `r nrow(data)` rows of data total. This is a small percent (`r nrow(df)/nrow(data)`) of the total data, but this should still be addressed. These values will be converted into NAs and what to do with them will be determined during model exploration. These rows of data could be removed from the data set completely, or a median length can be assumed, or potentially x can be calculated from depth and table percentage. Is it even worth addressing it all? This will be determined later in this project. 
<br /><br />

I also see zeros for y and z. The script below determines how many zeros there are for width (y):
```{r}
df <- data %>% 
  filter(y==0)

nrow(df)   # display how many rows are in object df
```
And for depth (z):
```{r}
df <- data %>% 
  filter(z==0)

nrow(df)
```
There are `r nrow(df)` missing data points for z. That is `r nrow(df)/nrow(data)` percent of the data. It's slightly more that x and y, but still a small sample. The script below will convert all zeros for x, y, and z into NAs, specifically numeric NAs.
```{r}   
data <- data %>%          # create a new object called data, from the old object called data
  mutate(                 # change the columns in the old object
    x = case_when(        # Change the column x by the following conditions
      x==0 ~ NA_real_,    # When x is 0, make it a NA
      TRUE ~ x),          # For everything else, keep x as is
    y = case_when(        # same logic as x
      y==0 ~ NA_real_,   
      TRUE ~ y),   
    z = case_when(        # same logic as x
      z==0 ~ NA_real_,
      TRUE ~ z))
```
All zeros are now NAs.

### Price vs Width (y)
Below is a scatter plot of price vs width (y).
```{r}
ggplot(data, aes(x=y, y=price))+
  geom_point(alpha=0.1)
```
<br />
First off, there are some outliers past a y of 20. Let's see what these points are.
```{r}
df <- data %>% 
  filter(y>20)

print(df)
```
These values for y can't be right. One way to double check is to use the equation for depth percentage: depth = 2 * z/(x + y). Using a little algebra we can solve for y: y = (2z/depth) - x. Before using these equations, we need a proof of concept. Depth and y will be calculated using these equations and we'll check if they equal the depth and y provided in the data. If they do, the equations are true.
```{r}
df <- data %>%
  select(depth, table, x, y, z) %>%   # Select only these columns from data
  mutate(
    depth_calc = round(((2*z)/(x+y))*100, # create a new column, depth_calc using the equation
                       digits = 1),       # round to the nearest one digit
    y_calc = round((2*z/(depth/100))-x,   # create a new column, y_calc using the equation
                   digits = 2),           # round to the nearest two digits
    depth_check = depth==depth_calc,      # create depth_check, which is true if depth equals depth_calc
    y_check = y==y_calc) %>%              # create y_check, which is true if y equals y_calc
    select(depth, depth_calc, depth_check, y, y_calc, y_check) # Select only these columns
  
head(df, n = 20) # show the first 20 rows
```
It looks like the equations work for the most part. In some instances they don't match 100%, but they are in the ballpark. This may be due to rounding errors. Now that these equations are confirmed. Let's calculate y for the two outliers.

```{r}
df <- data %>% 
  filter(y>20) %>% 
  mutate(y_calc = round((2*z/(depth/100))-x,
                   digits = 2))

print(df)
```
These values seem a lot more reasonable. Now let's commit this change to the data and re-plot.
```{r}
data <- data %>% 
  mutate(
    y = case_when(
      y>20 ~ round((2*z/(depth/100))-x,digits = 2),
      TRUE ~ y))
```
```{r}
ggplot(data, aes(x=y, y=price))+
  geom_point(alpha=0.1)
```
<br />
Those data points still look like they may be outliers but they are more reasonable than before, and trend is easier to see in this chart. Price increases at an increasing rate as y increases.

### Price vs Depth (z)
Below is a scatter plot of price vs depth (z).
```{r}
ggplot(data, aes(x=z, y=price))+
  geom_point(alpha=0.1)
```
<br />
Looks like there is an outlier past a depth (z) of 30. This may be a similar case as seen with width (y). Below shows this data point:
```{r}
df <- data %>% 
  filter(z>30)

print(df)
```
This doesn't pass the common sense test. Using the depth percentage equation we can calcuate depth (z) by using: z = depth*(x+y)/2. The results are below:
```{r}
df <- df %>% 
  mutate(
    z_calc = round(((depth/100)*(x+y))/2, digits = 2)
  )

print(df)
```
The calculated z looks more reasonable. The script below commits this change to the data and re-plots.
```{r}
data <- data %>% 
  mutate(
    z = case_when(
      z>30 ~ round(((depth/100)*(x+y))/2, digits = 2),
      TRUE ~ z
    )
  )
```
```{r}
ggplot(data, aes(x=z, y=price))+
  geom_point(alpha=0.1)
```
<br />
That's better! This trend appears to be the same as carat, x, and y. Now, onto some more detailed data exploration.


## Best of the Best
Best of the Best: Imagine someone wants to buy the best possible diamond, and money is no object. They only want to consider diamonds in the top categories of cut (Ideal), color (D), and clarity (IF). They want the most ideal range for depth (59-63) and table (54-57). Within the dataset, if we plot carat versus price can we fit a clean trendline? Is it linear? Exponential? What's the price of the largest carat, and is it the most expensive? <br /><br />

First we need to filter the data set to only the best cut, color, clarity, depth, and table.
```{r}
df <- data %>% 
  filter(
    cut=="Ideal",
    color=="D",
    clarity=="IF",
    between(depth, 59, 63),
    between(table, 54, 57))

print(df)
```
<br />
This filters the data to only `r nrow(df)` options. Let's plot carat versus price.
```{r}
ggplot(df, aes(x=carat, y=price)) +
  geom_point()
```
<br />
Looks like a linear model could be viable. Let's create one.
```{r}
model_df_lm <- lm(price ~ carat, df)

summary(model_df_lm)
```
<br />
Carat passes the T test (p-value) by a mile and the R squared isn't too bad. Let's plot this model now.
```{r}
pred_df_lm <- predict(model_df_lm, df)

df <- df %>% 
  mutate(pred = pred_df_lm)

ggplot(df, aes(x=carat, y=price)) +
  geom_point(color="blue") +
  geom_line(color="red", aes(y=pred))
```
<br />
Looks like the most expensive diamond might not be the largest one. Let's find these two data points.
```{r}
df2 <- df %>% 
  filter(
    carat == max(carat) |
      price == max(price)
  )

print(df2)
```
<br />
Interesting! The most expensive diamond is the not the largest diamond, but it has a larger depth and table compared to the largest carat diamond. This needs a closer look.

## Depth and Table
Depth and Table Percentages: I found the ideal depth and table values mentioned above online, but let's explore the dataset a little. if we fix the 4 C's (carat, cut, color, and clarity), how much do depth and table impact price? If we widen the ranges slightly, can we save a substantial amount? <br /><br />

First let's create a scatter plot of price vs carat.
```{r}
ggplot(data, aes(x=carat, y=price)) +
  geom_point()
```

<br />
Next, let's create a scatter plot of price vs depth.
```{r}
ggplot(data, aes(x=depth, y=price)) +
  geom_point()
```
<br />
Now let's create a scatter plot of price vs table.
```{r}
ggplot(data, aes(x=table, y=price)) +
  geom_point()
```

### Depth Deep Dive
A 3D scatter plot of price vs carat vs depth.
```{r}
plot_ly(data=data, x=~price, y=~carat, z=~depth, type = "scatter3d", marker = list(size=1)) %>% 
  layout(scene= list(xaxis=list(title="Price"), yaxis=list(title="Carat"), zaxis=list(title="depth")))
```
<br />
A 3D scatter plot of price vs carat vs depth with cut mapped to color.
```{r}
plot_ly(data=data, x=~price, y=~carat, z=~depth, color=~cut, type = "scatter3d", marker = list(size=1)) %>% 
  layout(scene= list(xaxis=list(title="Price"), yaxis=list(title="Carat"), zaxis=list(title="depth")))
```
<br />
A 3D scatter plot of price vs carat vs depth with color mapped to color.
```{r}
plot_ly(data=data, x=~price, y=~carat, z=~depth, color=~color, type = "scatter3d", marker = list(size=1)) %>% 
  layout(scene= list(xaxis=list(title="Price"), yaxis=list(title="Carat"), zaxis=list(title="depth")))
```
<br />
A 3D scatter plot of price vs carat vs depth with clarity mapped to color.
```{r}
plot_ly(data=data, x=~price, y=~carat, z=~depth, color=~clarity, type = "scatter3d", marker = list(size=1)) %>% 
  layout(scene= list(xaxis=list(title="Price"), yaxis=list(title="Carat"), zaxis=list(title="depth")))
```

### Table Deep Dive
A 3D scatter plot of price vs carat vs table.
```{r}
plot_ly(data=data, x=~price, y=~carat, z=~table, type = "scatter3d", marker = list(size=1)) %>% 
  layout(scene= list(xaxis=list(title="Price"), yaxis=list(title="Carat"), zaxis=list(title="Table")))
```
<br />
A 3D scatter plot of price vs carat vs table with cut mapped to color.
```{r}
plot_ly(data=data, x=~price, y=~carat, z=~table, color=~cut, type = "scatter3d", marker = list(size=1)) %>% 
  layout(scene= list(xaxis=list(title="Price"), yaxis=list(title="Carat"), zaxis=list(title="table")))
```
<br />
A 3D scatter plot of price vs carat vs table with color mapped to color.
```{r}
plot_ly(data=data, x=~price, y=~carat, z=~table, color=~color, type = "scatter3d", marker = list(size=1)) %>% 
  layout(scene= list(xaxis=list(title="Price"), yaxis=list(title="Carat"), zaxis=list(title="table")))
```
<br />
A 3D scatter plot of price vs carat vs table with clarity mapped to color.
```{r}
plot_ly(data=data, x=~price, y=~carat, z=~table, color=~clarity, type = "scatter3d", marker = list(size=1)) %>% 
  layout(scene= list(xaxis=list(title="Price"), yaxis=list(title="Carat"), zaxis=list(title="table")))
```
<br />

## Best Bang for the Buck
Best Bang for the Buck: Imagine someone wants to find the diamond which maximizes cut, color, and clarity per dollar. Using the expanded depth and table values from question 2 above, when does price start to increase exponentially for cut? What about for color? And clarity? <br /><br />

Under development.

## Bigger is Better
4. Bigger is Better: Imagine a guy named Bob who wants to buy a pair of diamonds for his wife, and have them made into earrings for her birthday. In Bob's mind, size (carat) is all that matters. He has $3200. He needs Two diamonds with the exact same cut, color, and clarity (with very comparable depth and table values), and he wants them to be as big as possible. What size carat can he afford? If he adjusts his budget, how much does the "maximum carat" size shift? Can we plot that and fit a line to it to find the "knee in the curve"?<br /><br />

Under development.

# Model Exploration
The script below summarizes all the data prep and cleaning steps in previous sections.
```{r}
# Load libraries needed for model exploration
library(tidyverse)      # for data manipulation
library(Metrics)        # to quickly develop metrics to determine accuracy of models
library(rpart)          # to create decision tree models
library(rpart.plot)     # to plot decision tree models
library(randomForest)   # to create random forest models
library(gbm)            # to create gradient boosted models
library(broom)          # to analyze model performance

# This script is to prep the data
# This way I don't have to re-run everything before this
data <- ggplot2::diamonds

# Reordering color so it's from worst to best. Just like all the other ordered features.
# This is personal preference. It's not a necessary step.
data <- data %>%                  # Create a new object, data, from the old object, data
  mutate(                         # Change its columns
    color = ordered(color,        # Make the color column and ordered column
                    levels = c(   # The order/levels are as follows:
                      "J",
                      "I",
                      "H",
                      "G",
                      "F",
                      "E",
                      "D"
                    )))

# Changing zeros into NAs
data <- data %>%          # create a new object called data, from the old object called data
  mutate(                 # change the columns in the old object
    x = case_when(        # Change the column x by the following conditions
      x==0 ~ NA_real_,    # When x is 0, make it a NA
      TRUE ~ x),          # For everything else, keep x as is
    y = case_when(        # same logic as x
      y==0 ~ NA_real_,   
      TRUE ~ y),   
    z = case_when(        # same logic as x
      z==0 ~ NA_real_,
      TRUE ~ z))

# Estimate y outliers
data <- data %>% 
  mutate(
    y = case_when(
      y>20 ~ round((2*z/(depth/100))-x,digits = 2),
      TRUE ~ y))

# Estimate z outliers
data <- data %>% 
  mutate(
    z = case_when(
      z>30 ~ round(((depth/100)*(x+y))/2, digits = 2),
      TRUE ~ z
    )
  )
```
One more step before we go into creating models. Let's create two random samples from the data. One sample will train the ML model, this is called the training data. And another smaller sample will be used to test the accuracy of the ML, this is called the testing data. Splitting up the data this way will allow for a fair comparison between models that doesn't allow an overfit model to have a competitive edge over other models.
```{r}
# Set the seed for reproducibility
set.seed(123)

# Create a random number vector from 1 to the number of rows in the data set.
# Make the length of the vector equal to about 80% of the data.
train_index <- sample(1:nrow(data), round(nrow(data)*0.8, digits = 0))

# subset the data into training data (80% of the data)
train <- data[train_index,]
# subset the data into testing data (the remaining 20%)
test <- data[-train_index,]
```


Now let's create some predictive machine learning models!

## Linear Regression
First, let's use a classic linear regression model. We'll start with a simple linear regression and then try some multivariate linear regression.

### Simple Linear Regression
Simple linear regression is your classic y=b+mx. The y is your dependent variable; the variable we wish to predict. In this case, it is price. The x is you independent variable; the feature that will predict the dependent variable. In this case, we'll start with carat which seemed to have a strong correlation with price. The m is your slope and the b is you y-intercept. The script below creates the simple linear model:
```{r}
# Set the seed for reproducibility
set.seed(123)

# Create an object called model_lm1 by using the lm fucntion
model_lm1 <- lm(price ~ carat,        # The formula is price as a function of carat
                train)                 # The data is an object called train

summary(model_lm1) # Stats summary of the model
```

The created formula is Price = `r model_lm1$coefficients[[1]]` + `r model_lm1$coefficients[[1]]` * Carat. Now, let's use this model to predict the values in the test data, and then analyze its effectiveness using three metrics: MAE, RMSE, and Adjusted R Square.
```{r}
# Create predictions for the test data using the simple linear model
predict_lm1 <- predict(model_lm1, test)

# calculate mean absolute error of model's predictions
mae_lm1 <- mae(test$price, predict_lm1)
mae_lm1

# calculate root mean squared error of model's predictions
rmse_lm1 <- rmse(test$price, predict_lm1)
rmse_lm1

# calculate r squared
SSR_lm1 <-sse(test$price, predict_lm1)      # residual sum of squares
SST_lm1 <-sse(test$price, mean(test$price)) # total sum of squares
Rsq_lm1 <- 1 - SSR_lm1/SST_lm1

# calculate adjusted r squared
RsqAdj_lm1 <- 1 - (1-Rsq_lm1)*((nrow(test)-1)/(nrow(test)-1-1))
RsqAdj_lm1
```
This model predicted the test data with a mean absolute error of `r mae_lm1`, a root mean squared error of `r rmse_lm1`, and a adjusted R square of `r RsqAdj_lm1`. Mean absolute error is the average of the absolute difference between actual values and the predictions. The root mean squared error is the square root of the squared difference between the predictions and actual values. The adjusted r square measures how well the actual outcomes are replicated by the regression and is adjusted (penalized) for how many independent variables are used for the model. Now let's build a multivariate linear regression and compare the results.


### Multivariate Linear Regression
#### Price vs All
The script below creates a multivariate linear regression model using all the potential variables in the data.
```{r}
# Set the seed for reproducibility
set.seed(123)

# Create an object called model_lm1 by using the lm fucntion
model_lm2 <- lm(price ~ .,        # The formula is price as a function of all other variables
                train,                # The data is an object called train
                na.action = na.omit)  # Exclude NAs               

summary(model_lm2) # Stats summary of the model
```

Now let's predict the testing data.
```{r}
# Create predictions for the test data using the simple linear model
predict_lm2 <- predict(model_lm2, test,
                       na.action = na.omit) # exclude NAs

# calculate mean absolute error of model's predictions
mae_lm2 <- mae(test$price, predict_lm2)
mae_lm2

# calculate root mean squared error of model's predictions
rmse_lm2 <- rmse(test$price, predict_lm2)
rmse_lm2

# calculate r squared
SSR_lm2 <-sse(test$price, predict_lm2)      # residual sum of squares
SST_lm2 <-sse(test$price, mean(test$price)) # total sum of squares
Rsq_lm2 <- 1 - SSR_lm2/SST_lm2

# calculate adjusted r squared
RsqAdj_lm2 <- 1 - (1-Rsq_lm2)*((nrow(test)-1)/(nrow(test)-1-1))
RsqAdj_lm2
```
Interesting! Adding more variables, led to a worse model. It has a higher MAE & RMSE, and a lower adjusted R Square. What if we only used the 4 C's.
#### Price vs 4 C's
```{r}
# Set the seed for reproducibility
set.seed(123)

# Create an object called model_lm1 by using the lm fucntion
model_lm3 <- lm(price ~ carat + cut + color + clarity,        # The formula is price as a function of all other variables
                train,                # The data is an object called train
                na.action = na.omit)  # Exclude NAs               

summary(model_lm3) # Stats summary of the model
```
```{r}
# Create predictions for the test data using the simple linear model
predict_lm3 <- predict(model_lm3, test,
                       na.action = na.omit) # exclude NAs

# calculate mean absolute error of model's predictions
mae_lm3 <- mae(test$price, predict_lm3)
mae_lm3

# calculate root mean squared error of model's predictions
rmse_lm3 <- rmse(test$price, predict_lm3)
rmse_lm3

# calculate r squared
SSR_lm3 <-sse(test$price, predict_lm3)      # residual sum of squares
SST_lm3 <-sse(test$price, mean(test$price)) # total sum of squares
Rsq_lm3 <- 1 - SSR_lm3/SST_lm3

# calculate adjusted r squared
RsqAdj_lm3 <- 1 - (1-Rsq_lm3)*((nrow(test)-1)/(nrow(test)-1-1))
RsqAdj_lm3
```
This is looking better! What about just depth and table? I have a hunch these two variables are going to be the worst predictors.

#### Price vs Depth and Table
```{r}
# Set the seed for reproducibility
set.seed(123)

# Create an object called model_lm1 by using the lm fucntion
model_lm4 <- lm(price ~ depth + table,        # The formula is price as a function of all other variables
                train,                # The data is an object called train
                na.action = na.omit)  # Exclude NAs               

summary(model_lm4) # Stats summary of the model
```
```{r}
# Create predictions for the test data using the simple linear model
predict_lm4 <- predict(model_lm4, test,
                       na.action = na.omit) # exclude NAs

# calculate mean absolute error of model's predictions
mae_lm4 <- mae(test$price, predict_lm4)
mae_lm4

# calculate root mean squared error of model's predictions
rmse_lm4 <- rmse(test$price, predict_lm4)
rmse_lm4

# calculate r squared
SSR_lm4 <-sse(test$price, predict_lm4)      # residual sum of squares
SST_lm4 <-sse(test$price, mean(test$price)) # total sum of squares
Rsq_lm4 <- 1 - SSR_lm4/SST_lm4

# calculate adjusted r squared
RsqAdj_lm4 <- 1 - (1-Rsq_lm4)*((nrow(test)-1)/(nrow(test)-1-1))
RsqAdj_lm4
```
Just as expected. This is not a good model at all! How about x, y, and z?

#### Price vs Dimensions (x, y, & z)
```{r}
# Set the seed for reproducibility
set.seed(123)

# Create an object called model_lm1 by using the lm fucntion
model_lm5 <- lm(price ~ x + y + z,        # The formula is price as a function of all other variables
                train,                # The data is an object called train
                na.action = na.omit)  # Exclude NAs               

summary(model_lm5) # Stats summary of the model
```
```{r}
# Create predictions for the test data using the simple linear model
predict_lm5 <- predict(model_lm5, test,
                       na.action = na.omit) # exclude NAs

# calculate mean absolute error of model's predictions
mae_lm5 <- mae(test$price, predict_lm5)
mae_lm5

# calculate root mean squared error of model's predictions
rmse_lm5 <- rmse(test$price, predict_lm5)
rmse_lm5

# calculate r squared
SSR_lm5 <-sse(test$price, predict_lm5)      # residual sum of squares
SST_lm5 <-sse(test$price, mean(test$price)) # total sum of squares
Rsq_lm5 <- 1 - SSR_lm5/SST_lm5

# calculate adjusted r squared
RsqAdj_lm5 <- 1 - (1-Rsq_lm5)*((nrow(test)-1)/(nrow(test)-1-1))
RsqAdj_lm5
```
Not the best, but does it help or hurt if we add it to the 4 C's?

#### Price vs 4 C's and Dimensions
```{r}
# Set the seed for reproducibility
set.seed(123)

# Create an object called model_lm1 by using the lm fucntion
model_lm6 <- lm(price ~ carat + cut + color + clarity + x + y + z,        # The formula is price as a function of all other variables
                train,                # The data is an object called train
                na.action = na.omit)  # Exclude NAs               

summary(model_lm6) # Stats summary of the model
```
```{r}
# Create predictions for the test data using the simple linear model
predict_lm6 <- predict(model_lm6, test,
                       na.action = na.omit) # exclude NAs

# calculate mean absolute error of model's predictions
mae_lm6 <- mae(test$price, predict_lm6)
mae_lm6

# calculate root mean squared error of model's predictions
rmse_lm6 <- rmse(test$price, predict_lm6)
rmse_lm6

# calculate r squared
SSR_lm6 <-sse(test$price, predict_lm6)      # residual sum of squares
SST_lm6 <-sse(test$price, mean(test$price)) # total sum of squares
Rsq_lm6 <- 1 - SSR_lm6/SST_lm6

# calculate adjusted r squared
RsqAdj_lm6 <- 1 - (1-Rsq_lm6)*((nrow(test)-1)/(nrow(test)-1-1))
RsqAdj_lm6
```
There's our answer. It hurts, doesn't help. It looks like the 4 C's are the best predictors. Now let's try a ML model other than linear regression.

## Random Forest
### Price vs 4 C's
Let's create a random forest model with the 4 C's.
```{r}
set.seed(123)

model_rf1 <- randomForest(price ~ carat + cut + color + clarity, 
                          train)
```
```{r}
model_rf1
```
```{r}
plot(model_rf1)
```
<br />
It looks like there 100 trees (iterations) are enough for this model.

```{r}
# Create predictions for the test data using the simple linear model
predict_rf1 <- predict(model_rf1, test)

# calculate mean absolute error of model's predictions
mae_rf1 <- mae(test$price, predict_rf1)
mae_rf1

# calculate root mean squared error of model's predictions
rmse_rf1 <- rmse(test$price, predict_rf1)
rmse_rf1

# calculate r squared
SSR_rf1 <-sse(test$price, predict_rf1)      # residual sum of squares
SST_rf1 <-sse(test$price, mean(test$price)) # total sum of squares
Rsq_rf1 <- 1 - SSR_rf1/SST_rf1

# calculate adjusted r squared
RsqAdj_rf1 <- 1 - (1-Rsq_rf1)*((nrow(test)-1)/(nrow(test)-1-1))
RsqAdj_rf1
```
It's good, but not as good as the linear regression. What if we use all the data?

### Price vs All
```{r}
set.seed(123)

model_rf2 <- randomForest(price ~ ., 
                          train,
                          na.action = na.omit, # remove NAs
                          ntree=300)           # only create 300 trees / iterations
```
```{r}
model_rf2
```
```{r}
plot(model_rf2)
```
<br />
It looks like there 100 trees (iterations) are enough for this model too.

```{r}
# Create predictions for the test data using the simple linear model
predict_rf2 <- predict(model_rf2, test)

# Create new tibble for test that includes predictions
# and filters out NAs
test_rf2 <- test %>% 
  mutate(pred = predict_rf2) %>% 
  filter(is.na(pred)==F)

# calculate mean absolute error of model's predictions
mae_rf2 <- mae(test_rf2$price, test_rf2$pred)
mae_rf2

# calculate root mean squared error of model's predictions
rmse_rf2 <- rmse(test_rf2$price, test_rf2$pred)
rmse_rf2

# calculate r squared
SSR_rf2 <-sse(test_rf2$price, test_rf2$pred)      # residual sum of squares
SST_rf2 <-sse(test_rf2$price, mean(test_rf2$price)) # total sum of squares
Rsq_rf2 <- 1 - SSR_rf2/SST_rf2

# calculate adjusted r squared
RsqAdj_rf2 <- 1 - (1-Rsq_rf2)*((nrow(test_rf2)-1)/(nrow(test_rf2)-1-1))
RsqAdj_rf2
```
This model out-performs the best linear model (Price vs The 4 C's). The RMSE is `r rmse_lm3` for the linear model, and `r rmse_rf2` for this random forest model. The adjusted R square is `r Rsq_lm3` for the linear model and `r Rsq_rf2` for the random forest model. However, the down side to the random forest model is that it can only use complete data. If there are NAs for x, y, and z, as we saw in the raw data, then this random forest model cannot predict price, and the linear model would have to be used.


## Regression Decision Tree
### Price vs 4 C's
```{r}
set.seed(123)

model_dt1 <- rpart(formula = price ~ cut + carat + color + clarity,
                   data = train,
                   method = "anova")
```
```{r}
print(model_dt1)
```
```{r}
rpart.plot(x=model_dt1,
           yesno = 2, type = 0, extra = 0) # personal preference on how the plot is displayed
```

```{r}
# Create predictions for the test data using the simple linear model
predict_dt1 <- predict(model_dt1, test)

# calculate mean absolute error of model's predictions
mae_dt1 <- mae(test$price, predict_dt1)
mae_dt1

# calculate root mean squared error of model's predictions
rmse_dt1 <- rmse(test$price, predict_dt1)
rmse_dt1

# calculate r squared
SSR_dt1 <-sse(test$price, predict_dt1)      # residual sum of squares
SST_dt1 <-sse(test$price, mean(test$price)) # total sum of squares
Rsq_dt1 <- 1 - SSR_dt1/SST_dt1

# calculate adjusted r squared
RsqAdj_dt1 <- 1 - (1-Rsq_dt1)*((nrow(test)-1)/(nrow(test)-1-1))
RsqAdj_dt1
```

### Price vs All
```{r}
set.seed(123)

model_dt2 <- rpart(formula = price ~ .,
                   data = train,
                   method = "anova")
```
```{r}
print(model_dt2)
```
```{r}
rpart.plot(x=model_dt2,
           yesno = 2, type = 0, extra = 0) # personal preference on how the plot is displayed
```

```{r}
# Create predictions for the test data using the simple linear model
predict_dt2 <- predict(model_dt2, test)

# calculate mean absolute error of model's predictions
mae_dt2 <- mae(test$price, predict_dt2)
mae_dt2

# calculate root mean squared error of model's predictions
rmse_dt2 <- rmse(test$price, predict_dt2)
rmse_dt2

# calculate r squared
SSR_dt2 <-sse(test$price, predict_dt2)      # residual sum of squares
SST_dt2 <-sse(test$price, mean(test$price)) # total sum of squares
Rsq_dt2 <- 1 - SSR_dt2/SST_dt2

# calculate adjusted r squared
RsqAdj_dt2 <- 1 - (1-Rsq_dt2)*((nrow(test)-1)/(nrow(test)-1-1))
RsqAdj_dt2
```

## Gradient Boosting Machine
### Price vs 4 C's
```{r}
set.seed(123)

model_gbm1 <- gbm(formula = price ~ carat + cut + color + clarity,
                  data = train,
                  distribution = "gaussian",
                  n.trees = 1000)
```
```{r}
print(model_gbm1)
```
```{r}
# Create predictions for the test data using the simple linear model
predict_gbm1 <- predict(model_gbm1, test)

# calculate mean absolute error of model's predictions
mae_gbm1 <- mae(test$price, predict_gbm1)
mae_gbm1

# calculate root mean squared error of model's predictions
rmse_gbm1 <- rmse(test$price, predict_gbm1)
rmse_gbm1

# calculate r squared
SSR_gbm1 <-sse(test$price, predict_gbm1)      # residual sum of squares
SST_gbm1 <-sse(test$price, mean(test$price)) # total sum of squares
Rsq_gbm1 <- 1 - SSR_gbm1/SST_gbm1

# calculate adjusted r squared
RsqAdj_gbm1 <- 1 - (1-Rsq_gbm1)*((nrow(test)-1)/(nrow(test)-1-1))
RsqAdj_gbm1
```

### Price vs All
```{r}
set.seed(123)

model_gbm2 <- gbm(formula = price ~ .,
                  data = train,
                  distribution = "gaussian",
                  n.trees = 1000)
```
```{r}
print(model_gbm2)
```
```{r}
# Create predictions for the test data using the simple linear model
predict_gbm2 <- predict(model_gbm2, test)

# calculate mean absolute error of model's predictions
mae_gbm2 <- mae(test$price, predict_gbm2)
mae_gbm2

# calculate root mean squared error of model's predictions
rmse_gbm2 <- rmse(test$price, predict_gbm2)
rmse_gbm2

# calculate r squared
SSR_gbm2 <-sse(test$price, predict_gbm2)      # residual sum of squares
SST_gbm2 <-sse(test$price, mean(test$price)) # total sum of squares
Rsq_gbm2 <- 1 - SSR_gbm2/SST_gbm2

# calculate adjusted r squared
RsqAdj_gbm2 <- 1 - (1-Rsq_gbm2)*((nrow(test)-1)/(nrow(test)-1-1))
RsqAdj_gbm2
```


### Price vs All



## Model Comparison
Under development.

# Discussion
Throughout this analysis some assumptions and data cleaning steps have been made. These decisions will impact how each predictive model will perform. A sensitivity analysis is recommended for the following data cleaning issues:
<br /><br />
1. Issue: x, y, and z values labeled as zeros. <br />
Action took in this analysis: Make all zeros NAs and ignore NAs in model exploration.<br />
<br />
2. Issue: Potential outliers for y and z. <br />
Action took in this analysis: Estimate y and z using the depth equation <br />

# Conclusion




